{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring L2B CH4 Plume Complexes\n",
    "\n",
    "**Summary**  \n",
    "\n",
    "In this notebook, we'll conduct a search and visualize the available methane plume complex observations, then select a region of interest (ROI) that shows several plumes that appear to be from the same source. We will then visualize a time series of plume data for the ROI, calculate the integrated methane enhancement for each plume. Lastly, to add more context to the plume complex timeseries, we will look at all EMIT acquisitions over the target regions and determine if there were factors such as clouds limiting plume detection, or simply no methane emissions during those acquisitions.\n",
    "\n",
    "> **Note**: Throughout this notebook, several complex functions and workflows are used to process and visualize data. These can be found in the `emit_tools.py` and `tutorial_utils.py` modules.\n",
    "\n",
    "**Background**\n",
    "\n",
    "The EMIT instrument is an imaging spectrometer that measures light in visible and infrared wavelengths. These measurements display unique spectral signatures that correspond to the composition on the Earth's surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. The EMIT instrument has also been used to successfully map methane point source emissions within its target mask. More details about EMIT and its associated products can be found in the **README.md** and on the [EMIT website](https://earth.jpl.nasa.gov/emit/).\n",
    "\n",
    "The L2B Estimated Methane Plume Complexes ([EMITL2BCH4PLM](https://lpdaac.usgs.gov/products/emitl2bch4plmv001/)) product provides estimated methane plume complexes in parts per million meter (ppm m) along with uncertainty data. This product is a plume specific subset of the EMIT L2B Methane Enhancement Data ([EMITL2BCH4ENH](https://lpdaac.usgs.gov/products/emitl2bch4enhv001/)) product. Each EMITL2BCH4PLM granule is sized to a specific plume complex but may cross multiple EMITL2BCH4ENH granules. A list of source EMITL2BCH4ENH granules is included in the GeoTIFF file metadata as well as the GeoJSON file. Each EMITL2BCH4PLM granule contains two files: one Cloud Optimized GeoTIFF (COG) file at a spatial resolution of 60 meters (m) and one GeoJSON file. The EMITL2BCH4PLM COG file contains a raster image of a methane plume complex extracted from EMITL2BCH4ENH v001 data. The EMITL2BCH4PLM GeoJSON file contains a vector outline of the plume complex, a list of source scenes, coordinates of the maximum enhancement values, and the uncertainty of the plume complex. \n",
    "\n",
    "The EMITL2BCH4ENH product only includes granules where methane plume complexes have been identified. To reduce the risk of false positives, all EMITL2BCH4ENH data undergo a manual review (or identification and confirmation) process before being designated as a plume complex. For more information on the manual review process, see Section 4.2.2 of the [EMIT GHG Algorithm Theoretical Basis Document (ATBD)](https://lpdaac.usgs.gov/documents/1696/EMIT_GHG_ATBD_V1.pdf). \n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "**Requirements** \n",
    " - Set up Python Environment - See **setup_instructions.md** in the `/setup/` folder\n",
    " - NASA Earthdata Login Account\n",
    "\n",
    "**Data Used** \n",
    " - [EMIT L2B Estimated Methane Plume Complexes (EMITL2BCH4PLM)](https://lpdaac.usgs.gov/products/emitl2bch4plmv001/)\n",
    " - [EMIT L2A Estimated Surface Reflectance and Uncertainty and Masks (EMITL2ARFL)](https://lpdaac.usgs.gov/products/emitl2arflv001/)\n",
    "\n",
    "**Learning Objectives** \n",
    " - Search for EMIT L2B Estimated Methane Plume Complexes\n",
    " - Visualize search results\n",
    " - Retrieve and visualize the EMIT L2B Estimated Methane Plume Complexes Metadata\n",
    " - Select a region of interest and build a time-series of plume data\n",
    " - Further investigate plume detection by looking at browse images and quality information\n",
    "\n",
    "**Tutorial Outline**  \n",
    "\n",
    "1. [**Search for EMIT L2B Estimated Methane Plume Complexes**](#search)\n",
    "2. [**Creating a Timeseries from Plume Data**](#plume-timeseries)\n",
    "3. [**Further investigation into plume detection**](#plume-detection)\n",
    "4. [**Calculating the Integrated Methane Enhancement for Plumes**](#ime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "\n",
    "from datetime import datetime\n",
    "import folium\n",
    "import earthaccess\n",
    "import folium.plugins\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "\n",
    "import hvplot\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "\n",
    "from branca.element import Figure\n",
    "from IPython.display import display\n",
    "from shapely.geometry.polygon import orient\n",
    "from shapely.geometry import Point\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "from emit_tools import emit_xarray, ortho_xr, ortho_browse\n",
    "from tutorial_utils import list_metadata_fields, results_to_geopandas, convert_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data we use or save will go into the `methane_tutorial/` directory, so we can go ahead and define that filepath now, relative to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methane_dir = '../../data/methane_tutorial/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Search for EMIT L2B Estimated Methane Plume Complexes<a id='search'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `earthaccess` to find all EMIT L2B Estimated Methane Plume Complexes (EMITL2BCH4PLM) data available from 2023. Define the date range, and concept-ids (unique product identifier) for the EMIT products that we want to search for, but leave the spatial arguments like `polygon` and `bbox` empty so we can preview detected methane plumes globally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collections for our search, using a dictionary\n",
    "concept_ids = {'plumes':'C2748088093-LPCLOUD', 'reflectance':'C2408750690-LPCLOUD', 'enhancement': 'C2748097305-LPCLOUD'}\n",
    "# Define Date Range\n",
    "date_range = ('2023-01-01','2023-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    concept_id=concept_ids['plumes'],\n",
    "    temporal=date_range,\n",
    "    count=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the results to a `geopandas.GeoDataFrame` using a function from our tutorials module. This gives a nice way to organize and visualize the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = results_to_geopandas(results)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default this function includes some fields, but you can add fields with a `fields` argument. To see all of the metadata available use the `list_metadata_fields` function imported from the `tutorial_utils.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "list_metadata_fields(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an index column to the dataframe to include it in the tooltips for our visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify index so we can reference it with gdf.explore()\n",
    "gdf['index']=gdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Figure and Basemap tiles\n",
    "fig = Figure(width=\"1080px\",height=\"540\")\n",
    "map1 = folium.Map(tiles=None)\n",
    "folium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\n",
    "folium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n",
    "                name='ESRI World Imagery',\n",
    "                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n",
    "                overlay='True').add_to(map1)\n",
    "fig.add_child(map1)\n",
    "# Add Search Results gdf\n",
    "gdf.explore(\"_single_date_time\",\n",
    "            categorical=True,\n",
    "            style_kwds={\"fillOpacity\":0.1,\"width\":2},\n",
    "            name=\"EMIT L2B CH4PLM\",\n",
    "            tooltip=[\n",
    "                \"index\",\n",
    "                \"native-id\",\n",
    "                \"_single_date_time\",\n",
    "            ],\n",
    "            m=map1,\n",
    "            legend=False\n",
    ")\n",
    "\n",
    "# Zoom to Data\n",
    "map1.fit_bounds(bounds=convert_bounds(gdf.unary_union.bounds))\n",
    "# Add Layer controls\n",
    "map1.add_child(folium.LayerControl(collapsed=False))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll choose a region that looks like it has a several plumes being emitted from the same source, a landfill in Jordan. To create a simple bounding box around our target region, we can just use the plumes that extend furthest in the cardinal directions to generate a bounding box around the region that we can use in our upcoming analysis. \n",
    "\n",
    "Create a list of these plumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note these index values can change if new data is added\n",
    "plumes =[143,191,235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our list of plumes to index our `GeoDataFrame` (gdf) and create a bounding box enveloping those geometries, then create a new `GeoDataFrame` with our new bounding box as the geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = gdf.loc[plumes].geometry.unary_union.envelope\n",
    "bbox = orient(bbox, sign=1)\n",
    "plume_bbox = gpd.GeoDataFrame({\"name\":['plume_bbox'], \"geometry\":[bbox]},crs=gdf.crs)\n",
    "plume_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize our selected region, plumes, and bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Figure and Basemap tiles\n",
    "fig = Figure(width=\"1080px\",height=\"540\")\n",
    "map1 = folium.Map(tiles=None)\n",
    "folium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\n",
    "folium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n",
    "                name='ESRI World Imagery',\n",
    "                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n",
    "                overlay='True').add_to(map1)\n",
    "fig.add_child(map1)\n",
    "# Add Search Results gdf\n",
    "plume_bbox.explore(\"name\",\n",
    "                   name='Plume BBox',\n",
    "                   style_kwds={\"fillOpacity\":0,\"width\":2},\n",
    "                   m=map1,\n",
    "                   legend=False)\n",
    "\n",
    "gdf.explore(\"_single_date_time\",\n",
    "            categorical=True,\n",
    "            style_kwds={\"fillOpacity\":0.1,\"width\":2},\n",
    "            name=\"EMIT L2B CH4PLM\",\n",
    "            tooltip=[\n",
    "                \"index\",\n",
    "                \"native-id\",\n",
    "                \"_single_date_time\",\n",
    "            ],\n",
    "            m=map1,\n",
    "            legend=False\n",
    ")\n",
    "# Zoom to Data\n",
    "map1.fit_bounds(bounds=convert_bounds(plume_bbox.unary_union.bounds))\n",
    "# Add Layer controls\n",
    "map1.add_child(folium.LayerControl(collapsed=False))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save our bounding box to a `geojson` file if desired using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_bbox.to_file(f'{methane_dir}jordan_plumes_bbox.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset our geodataframe of plumes to only those that intersect our bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_gdf = gdf[gdf.geometry.intersects(plume_bbox.geometry[0])]\n",
    "plm_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_gdf['_related_urls'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve additional plume metadata contained in the EMIT L2B Estimated Methane Plume Complexes (EMITL2BCH4PLM) data product, which contains the maximum enhancement value, the uncertainty of the plume complex, and the list of source scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asset_url(row,asset, key='Type',value='GET DATA'):\n",
    "    \"\"\"\n",
    "    Retrieve a url from the list of dictionaries for a row in the _related_urls column.\n",
    "    Asset examples: CH4PLM, CH4PLMMETA, RFL, MASK, RFLUNCERT \n",
    "    \"\"\"\n",
    "    # Add _ to asset so string matching works\n",
    "    asset = f\"_{asset}_\"\n",
    "    # Retrieve URL matching parameters\n",
    "    for _dict in row['_related_urls']:\n",
    "        if _dict.get(key) == value and asset in _dict['URL'].split('/')[-1]:\n",
    "            return _dict['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Improve implementation using asyncio/aiohttp\n",
    "def fetch_ch4_metadata(row):\n",
    "    response = requests.get(get_asset_url(row, 'CH4PLMMETA'))\n",
    "    return response.json()['features'][0]['properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row and convert the result to a DataFrame\n",
    "# plm_meta = plm_gdf.apply(fetch_ch4_metadata, axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the points with highest methane concentration to our visualization.\n",
    "\n",
    "Create an index column, as we did for the plumes, then convert the latitude and longitude of max concentration to a shapely `Point` object and add it to our `GeoDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify index so we can reference it with gdf.explore()\n",
    "# plm_meta['index'] = plm_meta.index\n",
    "# # Add Geometry and convert to geodataframe\n",
    "# plm_meta['geometry'] = plm_meta.apply(lambda row: Point(row['Longitude of max concentration'], row['Latitude of max concentration']), axis=1)\n",
    "# plm_meta = gpd.GeoDataFrame(plm_meta, geometry='geometry', crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plm_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add this to our visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up Figure and Basemap tiles\n",
    "# fig = Figure(width=\"1080px\",height=\"540\")\n",
    "# map1 = folium.Map(tiles=None)\n",
    "# folium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\n",
    "# folium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n",
    "#                 name='ESRI World Imagery',\n",
    "#                 attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n",
    "#                 overlay='True').add_to(map1)\n",
    "# fig.add_child(map1)\n",
    "# # Add Search Results gdf\n",
    "# plume_bbox.explore(\"name\",\n",
    "#                    name='Plume BBox',\n",
    "#                    style_kwds={\"fillOpacity\":0,\"width\":2},\n",
    "#                    m=map1,\n",
    "#                    legend=False)\n",
    "\n",
    "# plm_gdf.explore(\"index\",\n",
    "#             categorical=True,\n",
    "#             style_kwds={\"fillOpacity\":0.1,\"width\":2},\n",
    "#             name=\"EMIT L2B CH4PLM\",\n",
    "#             tooltip=[\n",
    "#                 \"index\",\n",
    "#                 \"native-id\",\n",
    "#                 \"_single_date_time\",\n",
    "#             ],\n",
    "#             m=map1,\n",
    "#             legend=False\n",
    "# )\n",
    "\n",
    "# plm_meta.explore(\"index\",\n",
    "#             categorical=True,\n",
    "#             style_kwds={\"fillOpacity\":0.1,\"width\":2},\n",
    "#             name=\"Location of Max Concentration (ppm m)\",\n",
    "#             tooltip=[\n",
    "#                 \"DAAC Scene Names\",\n",
    "#                 \"UTC Time Observed\",\n",
    "#                 \"Max Plume Concentration (ppm m)\",\n",
    "#                 \"Concentration Uncertainty (ppm m)\",\n",
    "#                 \"Orbit\"\n",
    "#             ],\n",
    "#             m=map1,\n",
    "#             legend=False\n",
    "# )\n",
    "# # Zoom to Data\n",
    "# map1.fit_bounds(bounds=convert_bounds(plume_bbox.unary_union.bounds))\n",
    "# # Add Layer controls\n",
    "# map1.add_child(folium.LayerControl(collapsed=False))\n",
    "# display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Timeseries from Plume Data<a id='plume-timeseries'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize a timeseries of these plumes that appear to be from the same source. To do this we'll generate a list of the COG urls for the plumes, then use rioxarray to build a timeseries based on the dates in the filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows in the plm_gdf and get the CH4PLM urls and store them in a list\n",
    "plm_urls = plm_gdf.apply(lambda row: get_asset_url(row, asset='CH4PLM'), axis=1).tolist()\n",
    "plm_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of COG urls we can set our `gdal` configuration options to pass our NASA Earthdata login credentials when we access each COG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDAL configurations used to successfully access LP DAAC Cloud Assets via vsicurl \n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF')\n",
    "gdal.SetConfigOption('GDAL_HTTP_UNSAFESSL', 'YES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the plumes have different extents and some are from the same scene, our first step to create a timeseries will be to create a standard grid to project all of our plumes onto. We'll use the first plume in our list to define the resolution of the grid, and our bounding box to define the extent.\n",
    "\n",
    "Open the first plume in our list of urls and squeeze the `band` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm = rxr.open_rasterio(plm_urls[0], masked=True).squeeze('band',drop=True)\n",
    "plm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO - is this the best way to do this?\n",
    "\n",
    "Create a new `xarray.DataArray` to represent our extent and resolution that we match all of our others to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_extent = plume_bbox.total_bounds #[minx, miny, maxx, maxy]\n",
    "# Calculate new raster shape using the new extent, maintaining the original resolution\n",
    "height = int(np.ceil((new_extent[3] - new_extent[1]) / abs(plm.rio.resolution()[1])))\n",
    "width = int(np.ceil((new_extent[2] - new_extent[0]) / plm.rio.resolution()[0]))\n",
    "data = np.full((height,width),plm.rio.nodata)\n",
    "coords = {'y':(['y'],np.arange(new_extent[1], new_extent[3], abs(plm.rio.resolution()[1]))),\n",
    "          'x':(['x'],np.arange(new_extent[0], new_extent[2], plm.rio.resolution()[0]))}\n",
    "to_match = xr.DataArray(data, coords=coords)\n",
    "to_match.rio.write_crs(plm.rio.crs, inplace=True)\n",
    "to_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del plm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our goal is to open the plumes from each url in our list, merge plumes acquired at the same time, then concatenating all of the plumes along a time dimension based on the acquisition time in the granule name.\n",
    "\n",
    "To do this we will loop over our list of urls, open each plume and reproject it onto our `to_match` grid, merge plumes acquired at the same time, and store them in a dictionary where keys correspond to the acquisition time and values are the plume data in an `xarray.DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_ts_dict = {}\n",
    "# Set max retries for vsicurl errors\n",
    "max_retries=5\n",
    "# Iterate over plm urls\n",
    "for url in plm_urls:\n",
    "    # retrieve acquisition time from url\n",
    "    acquisition_time = url.split('/')[-1].split('.')[-2].split('_')[-2]\n",
    "    # list plumes identified in same scene if there are any\n",
    "    same_scene = [url for url in plm_urls if acquisition_time in url.split('/')[-1].split('.')[-2].split('_')[-2]]\n",
    "    to_merge = []\n",
    "    # prevent duplicate processing of plumes from the same scene\n",
    "    if acquisition_time not in list(plm_ts_dict.keys()):\n",
    "        # Open and merge plumes identified from each scene\n",
    "        for _plm in same_scene:\n",
    "            print(f\"Opening {_plm.split('/')[-1]}\")\n",
    "            # Try loop for vsicurl/unrecongnized format error\n",
    "            for retry in range(max_retries):\n",
    "                try:\n",
    "                    # Open COG and squeeze band dimension\n",
    "                    plm = rxr.open_rasterio(_plm).squeeze('band', drop=True)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f'{e} Retrying...')\n",
    "                else:\n",
    "                    print(f\"Failed to process {url} after {max_retries} retries. Please check to see you're authenticated with earthaccess.\")\n",
    "            # Add to list of plumes to merge\n",
    "            to_merge.append(plm)\n",
    "            # Merge plumes and add to timeseries we also need to use `reproject_match` here in case a plume extends outside of our bounding box, because of our lazy bbox construction.\n",
    "            plm_ts_dict[acquisition_time] = rxr.merge.merge_arrays(to_merge,bounds=to_match.rio.bounds()).rio.reproject_match(to_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our plumes on a standard grid, we can concatenate them along a time dimension to create a timeseries. Create an xarray variable called 'time' from our dictionary keys, then use `xarray.concat` to concatenate all of our plumes along the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_time = xr.Variable('time', [datetime.strptime(t,'%Y%m%dT%H%M%S') for t in list(plm_ts_dict.keys())])\n",
    "plm_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_ts_ds = xr.concat(list(plm_ts_dict.values()), dim=plm_time)\n",
    "plm_ts_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_ts_ds.data[plm_ts_ds.data == -9999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_ts_ds.hvplot.image(x='x',y='y',geo=True, tiles='ESRI', crs='EPGS:4326', cmap='inferno',clim=(np.nanmin(plm_ts_ds.data),np.nanmax(plm_ts_ds.data)),clabel=f'Methane Concentation ({plm_ts_ds.Units})', frame_width=600, frame_height=600, rasterize=True)*plume_bbox.hvplot(color='red',crs='EPSG:4326',fill_color=None, line_color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Further investigation into plume detection<a id='plume-detection'></a>\n",
    "\n",
    "The time-series shown above doesn't necessarily give us a full a full picture of what's happening. Although this plume is fairly persistent, absence of observations does not mean the source isn't emitting methane. Since EMIT is on the ISS, the revisit period varies, limiting the number of observations we can use. Additionally, clouds and other obstructions can limit plume detection. To add more context to the plume complex timeseries, we will look at all EMIT acquisitions over the target regions and try to determine if there were factors affecting plume detection, or simply no methane emissions during those acquisitions.\n",
    "\n",
    "Create an ROI for a new search from our bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = list(plume_bbox.geometry[0].exterior.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now conduct a search for reflectance data over our ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfl_results = earthaccess.search_data(\n",
    "    concept_id=concept_ids['reflectance'],\n",
    "    polygon=roi,\n",
    "    temporal=date_range,\n",
    "    count=2000\n",
    ")\n",
    "rfl_gdf = results_to_geopandas(rfl_results)\n",
    "rfl_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the footprints of these scenes to gain some insight into coverage over our ROI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Figure and Basemap tiles\n",
    "fig = Figure(width=\"1080px\",height=\"540\")\n",
    "map1 = folium.Map(tiles=None)\n",
    "folium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\n",
    "folium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n",
    "                name='ESRI World Imagery',\n",
    "                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n",
    "                overlay='True').add_to(map1)\n",
    "fig.add_child(map1)\n",
    "\n",
    "# Add Search Reflectance Scenes with no CH4\n",
    "rfl_gdf.explore(color='red',\n",
    "               style_kwds={\"fillOpacity\":0,\"width\":2},\n",
    "               name=\"Scenes with no CH4 Plumes\",\n",
    "               tooltip=[\n",
    "                \"native-id\",\n",
    "                \"_beginning_date_time\",\n",
    "                ],\n",
    "                m=map1,\n",
    "                legend=False)\n",
    "\n",
    "# Add Plume BBox to Map\n",
    "plume_bbox.explore(m=map1,\n",
    "                   name='Plumes Bounding Box',\n",
    "                   legend=False)\n",
    "\n",
    "# Zoom to Data\n",
    "map1.fit_bounds(bounds=convert_bounds(rfl_gdf.unary_union.bounds))\n",
    "# Add Layer controls\n",
    "map1.add_child(folium.LayerControl(collapsed=False))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there are several scenes that intersect with our ROI, but likely have no relevant information since they only cover a small portion or corner of the ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar process as with the methane product to construct a time series to better understand the data gathered on each overpass. To do this, we will use the browse imagery and the masks included in the EMITL2ARFL product. The by default the mask files and browse images are not orthorectified, so we must do that as part of our workflow.\n",
    "\n",
    "First, get the urls for the browse images and masks for each scene in our `rfl_gdf` search results using the `get_asset_urls` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_urls = rfl_gdf.apply(lambda row: get_asset_url(row, asset='RFL', value='GET RELATED VISUALIZATION'), axis=1).tolist()\n",
    "png_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_urls = rfl_gdf.apply(lambda row: get_asset_url(row, asset='MASK'), axis=1).tolist()\n",
    "mask_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write these as a text file so we don't need to search again, although we will use the `rfl_gdf` GeoDataFrame later in the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save URL List\n",
    "# with open(f'{methane_dir}rfl_mask_urls.txt', 'w') as f:\n",
    "#     for line in mask_urls:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mask files are not chunked, its quicker to download them to do the processing.\n",
    "\n",
    "Login with `earthaccess` and download these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.login(persist=True)\n",
    "# Get requests https Session using Earthdata Login Info\n",
    "fs = earthaccess.get_requests_https_session()\n",
    "# Retrieve granule asset ID from URL (to maintain existing naming convention)\n",
    "for url in mask_urls:\n",
    "    granule_asset_id = url.split('/')[-1]\n",
    "    # Define Local Filepath\n",
    "    fp = f'{methane_dir}{granule_asset_id}'\n",
    "    # Download the Granule Asset if it doesn't exist\n",
    "    if not os.path.isfile(fp):\n",
    "        with fs.get(url,stream=True) as src:\n",
    "            with open(fp,'wb') as dst:\n",
    "                for chunk in src.iter_content(chunk_size=64*1024*1024):\n",
    "                    dst.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these scenes we want to open EMIT L2A Mask data, then subset spatially and select only the variable we want, in this case we'll use the `Aggregate flag` from the `masks` dataarray. The `Aggregate flag` includes the cloud mask, dilated cloud mask, cirrus mask, water mask, and an AOD threshold of 0.5. We can assume most of these will affect the methane plume detection.\n",
    "\n",
    "As we do this, we will also take the GLT, read in the browse image, clip, and orthorectify it to our ROI. We can do this because the browse png files are in the native resolution and can be broadcast onto an orthorectified grid using the GLT.\n",
    "\n",
    "First, get the filepaths for our downloaded mask data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the downloaded files\n",
    "fps = glob.glob(f'{methane_dir}*.nc')\n",
    "fns = [os.path.basename(fp) for fp in fps]\n",
    "fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to loop through our files, orthorectifying the mask and browse image, clipping to our ROI and reprojecting to our predefined `to_match` grid, and finally saving outputs as a COG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenes(fns, outdir, gdf):\n",
    "    \"\"\"\n",
    "    This function will process a list of EMIT Mask scenes, downloading, merging adjacent scenes and building a time-series, as well as orthorectifying/m and returning the browse images.\n",
    "    \"\"\"\n",
    "    for fn in fns:\n",
    "        # Get Granule Asset ID for First Adjacent Scene (may only be one)\n",
    "        granule_asset_id = fn.split('.')[-2]\n",
    "        # Set Output Path\n",
    "        outpath_mask = f\"{outdir}{granule_asset_id}_aggregate_flag.tif\"\n",
    "        outpath_browse = f\"{outdir}{granule_asset_id}_ortho_browse.tif\"\n",
    "        # Check if the file exists\n",
    "        if not os.path.isfile(outpath_mask):\n",
    "            # Open Mask Dataset\n",
    "            emit_ds = emit_xarray(f'{methane_dir}{fn}', ortho=False)\n",
    "            # Retrieve GLT, spatial_ref, and geotransform to use on browse image\n",
    "            glt = np.nan_to_num(np.stack([emit_ds[\"glt_x\"].data, emit_ds[\"glt_y\"].data], axis=-1),nan=0).astype(int)\n",
    "            spatial_ref = emit_ds.spatial_ref\n",
    "            gt = emit_ds.geotransform\n",
    "            # Select browse image url corresponding to the scene\n",
    "            png_url = [url for url in png_urls if fn.split('.')[-2].split('_')[-3] in url][0]\n",
    "            # Orthorectify browse and mask\n",
    "            rgb = ortho_browse(png_url, glt, spatial_ref, gt)\n",
    "            emit_ds = ortho_xr(emit_ds)\n",
    "            # Clip to Geometry using rioxarray\n",
    "            emit_ds = emit_ds.rio.clip(gdf.geometry.values,gdf.crs, all_touched=True)          \n",
    "            rgb = rgb.rio.clip(gdf.geometry.values,gdf.crs, all_touched=True)\n",
    "            # Select only mask array and desired quality flag and reproject to match our chosen extent\n",
    "            mask_da = emit_ds['mask'].sel(mask_bands='Aggregate Flag')\n",
    "            # Drop elevation\n",
    "            mask_da = mask_da.drop_vars('elev')\n",
    "            mask_da.name = 'Aggregate Flag'\n",
    "            mask_da.data = np.nan_to_num(mask_da.data, nan=-9999)\n",
    "            mask_da = mask_da.rio.reproject_match(to_match, nodata=-9999)\n",
    "            #mask_da.rio.write_nodata(np.nan, inplace=True)\n",
    "            # Reproject rgb\n",
    "            rgb = rgb.rio.reproject_match(to_match, nodata=0)\n",
    "            # Write cog outputs        \n",
    "            mask_da.rio.to_raster(outpath_mask,driver=\"COG\")\n",
    "            rgb.rio.to_raster(outpath_browse,driver=\"COG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_scenes(fns, methane_dir, plume_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of the processed files to use in creation of a timeseries. We'll use a similar process to what we did for the plumes, adding a `time` variable to our datasets and concatenating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_files = sorted(glob.glob(f'{methane_dir}*aggregate_flag.tif'))\n",
    "mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_files = sorted(glob.glob(f'{methane_dir}*ortho_browse.tif'))\n",
    "rgb_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a time index from the filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_index_from_filenames(file_names,datetime_pos):\n",
    "    \"\"\"\n",
    "    Helper function to create a pandas DatetimeIndex\n",
    "    \"\"\"\n",
    "    return [datetime.strptime(f.split('_')[datetime_pos], '%Y%m%dT%H%M%S') for f in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_time = xr.Variable('time', time_index_from_filenames(mask_files, -5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and concatenate our datasets along the time dimension, then assign fill_values to `np.nan` to make those sections of the data transparent in our visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_ts_da = xr.concat([rxr.open_rasterio(f).squeeze('band', drop=True).rio.reproject_match(to_match) for f in mask_files], dim=mask_time)\n",
    "quality_ts_da.data[quality_ts_da.data <= 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add a column to our plume geodataframe containing a time index formatted similarly to our quality `time` dimension so we can visualize our plume extents with our quality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm_gdf['time'] = pd.to_datetime(plm_gdf.loc[:,'_single_date_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the quality timeseries, bounding box, and plume extents on the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_ts_map = quality_ts_da.hvplot.image(x='x',y='y',cmap='greys',groupby='time',clim=(0,1),geo=True,frame_height=400, alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB images are a good way to add something more visually understandable than just the mask layers. Follow the same process as above to build an RGB timeseries, then plot it with the bounding box and plume extents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_ts_ds = xr.concat([rxr.open_rasterio(f).rio.reproject_match(to_match) for f in rgb_files], dim=mask_time)\n",
    "rgb_ts_ds.data[rgb_ts_ds.data == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_ts_map = rgb_ts_ds.hvplot.rgb(x='x',y='y', bands='band',groupby='time',geo=True, frame_height=400, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_ts_map*quality_ts_map*plm_gdf.hvplot(groupby='time', geo=True, line_color='red', fill_color=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this timeseries that most overpasses we included did not cover enough of the region to capture a plume if one was being emitted. If we intend to do further analysis, we can omit these, or perhaps if conducting a similar analysis in the future, we choose a smaller more specific bounding polygon for our search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculating the IME for each plume<a id='ime'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ kg\\ \\ (per \\ \\ pixel) = \\frac{ppm \\cdot m}{1} \\frac{1}{1 \\cdot 10^6 \\ \\ ppm} \\frac {60 \\ \\ m \\cdot 60 \\ \\ m} {1} \\frac {1000 \\ \\ L} {m^3} \\frac {1 \\ \\ mol} {22.4 \\ \\ L} \\frac {0.01604 \\ \\ kg} {1 \\ \\ mol}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ime(plume_da):\n",
    "    molar_volume = 22.4 # L/mol at STP\n",
    "    molar_mass_ch4 = 0.01604 #kg/mol\n",
    "\n",
    "    kg = plume_da * (1/1e6) * (60*60) * (1000) * (1/molar_volume) * molar_mass_ch4\n",
    "    ime = np.nansum(kg)\n",
    "    return ime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function along the 'x' and 'y' dimensions\n",
    "ime_ts = xr.apply_ufunc(calc_ime, plm_ts_ds, input_core_dims=[['y', 'x']], vectorize=True)\n",
    "ime_ts.name = 'IME (kg)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ime_ts.hvplot.line(x='time',y='IME (kg)', title='Observed Integrated Methane Emissions (kg) over 2023', color='black', xticks=list(ime_ts.time.data), rot=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
