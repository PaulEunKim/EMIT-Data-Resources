{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Finding EMIT L2B Mineralogy Data\n",
    "\n",
    "**Summary**\n",
    "\n",
    "#TODO\n",
    " - Add uncertainty to the outputs\n",
    " - better describe some cells\n",
    " - add image to this cell\n",
    "\n",
    "<div>\n",
    "<img src=\"../img/mineralogy_search.png\" width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "**Background**\n",
    "\n",
    "**Requirements**\n",
    "\n",
    " - [NASA Earthdata Account](https://urs.earthdata.nasa.gov/home)   \n",
    " - *No Python setup requirements if connected to the workshop cloud instance!*  \n",
    " - **Local Only** Set up Python Environment - See **setup_instructions.md** in the `/setup/` folder to set up a local compatible Python environment \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "**Tutorial Outline**\n",
    "\n",
    "1. Setup  \n",
    "2. Searching for EMIT L2B Mineralogy Data\n",
    "3. Advanced Filtering\n",
    "4. Visualizing Data\n",
    "5. Creating a List of Results and Asset URLs\n",
    "6. Streaming or Downloading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import folium\n",
    "import earthaccess\n",
    "import warnings\n",
    "import folium.plugins\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import math\n",
    "\n",
    "from branca.element import Figure\n",
    "from IPython.display import display\n",
    "from shapely import geometry\n",
    "from skimage import io\n",
    "from datetime import timedelta\n",
    "from shapely.geometry.polygon import orient\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "from emit_tools import emit_xarray, ortho_xr, ortho_browse\n",
    "from tutorial_utils import list_metadata_fields, results_to_geopandas, convert_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 NASA Earthdata Login Credentials\n",
    "\n",
    "To download or stream NASA data you will need an Earthdata account, you can create one [here](https://urs.earthdata.nasa.gov/home). Searching We will use the `login` function from the `earthaccess` library for authentication before downloading at the end of the notebook. This function can also be used to create a local `.netrc` file if it doesn't exist or add your login info to an existing `.netrc` file. If no Earthdata Login credentials are found in the `.netrc` you'll be prompted for them. This step is not necessary to conduct searches but is needed to download or stream data.\n",
    "\n",
    "## 2.0 Searching for EMIT L2B Mineralogy Data\n",
    "\n",
    "To find data we will use the [`earthaccess` Python library](https://github.com/nsidc/earthaccess). `earthaccess` searches [NASA Common Metadata Repository (CMR) API](), a metadata system that catalogs Earth Science data and associated metadata records. The results can then be used to download granules or generate lists of granule search result URLs.\n",
    "\n",
    "Using `earthaccess` we can search based on the attributes of a granule, which can be thought of as a spatiotemporal scene from an instrument containing multiple assets (ex: Reflectance, Reflectance Uncertainty, Masks for the EMIT L2A Reflectance Collection, and EMIT ). When conducting a search we can provide a product, in this case the mineralogy product, a date-time range, and spatial constraints. This process can also be used with other EMIT products, other NASA collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Querying for Datasets\n",
    "\n",
    "Our first step in searching for data is determining which collection (e.g. EMIT L2A Estimated Surface Reflectance Uncertainty and Masks, EMIT L2B Estimated Mineral Identification and Band Depth and Uncertainty) we want to search for. The best way to do this is using the collection `short_name` (e.g. EMITL2ARFL, EMITL2BMIN) or `concept-id`. In rare cases, the `short_name` of two collections can be the same, so we will use the `concept-id` which is a unique identifier for each collection. To find the `concept-id` we can search using some keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMIT Collection Query\n",
    "emit_collection_query = earthaccess.collection_query().keyword('EMIT L2B Mineral')\n",
    "emit_collection_query.fields(['ShortName','EntryTitle','Version']).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this list of results we can see that the `concept-id` for the desired mineral product is `C2408034484-LPCLOUD`. We can use this to define one of our search arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_id = 'C2408034484-LPCLOUD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Temporal Range\n",
    "\n",
    "For our date range, we'll look at all EMIT data collected over 2023. The `date_range` can be specified as a pair of dates, start and end (up to, not including)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = ('2023-01-01','2024-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define Spatial Region of Interest\n",
    "\n",
    "For this example, our spatial region of interest will be the area around Cuprite, NV. A location where there have been several previous mineralogy studies. We can define this region using a rectangular polygon. If you want to make a polygon for a different region, you can use a tool like [geojson.io](http://geojson.io/).\n",
    "\n",
    "Open the `geojson` as a `geodataframe`, and check the coordinate reference system (CRS) of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_gdf = gpd.read_file('../../data/cuprite_bbox.geojson')\n",
    "roi_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this `geodataframe` consists of a single polygon which we want to include in our search, but the geometry is the only information contained in the file, so lets add a column for the site name, and set the value to \"Cuprite\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_gdf['Name'] = 'Cuprite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a bounding box for use in leaflet notation\n",
    "\n",
    "def convert_bounds(bbox, invert_y=False):\n",
    "    \"\"\"\n",
    "    Helper method for changing bounding box representation to leaflet notation\n",
    "\n",
    "    ``(lon1, lat1, lon2, lat2) -> ((lat1, lon1), (lat2, lon2))``\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    if invert_y:\n",
    "        y1, y2 = y2, y1\n",
    "    return ((y1, x1), (y2, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure(width=\"750px\", height=\"375px\")\n",
    "map1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\n",
    "fig.add_child(map1)\n",
    "\n",
    "# Add roi geodataframe\n",
    "roi_gdf.explore(\n",
    "    \"Name\",\n",
    "    popup=True,\n",
    "    categorical=True,\n",
    "    cmap='Set3',\n",
    "    style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n",
    "    name=\"Regions of Interest\",\n",
    "    m=map1\n",
    ")\n",
    "\n",
    "map1.add_child(folium.LayerControl())\n",
    "map1.fit_bounds(bounds=convert_bounds(roi_gdf.unary_union.bounds))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `earthaccess` search, we will use the `polygon` argument to find where this geometry intersects with the footprint of the EMIT scenes. To do this, we need to create a list of exterior polygon vertices in counter-clockwise order to submit in our search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use orient to place vertices in counter-clockwise order\n",
    "roi = orient(roi_gdf.geometry[0], sign = 1.0)\n",
    "# Put the exterior coordinates in a list\n",
    "roi = list(roi.exterior.coords)\n",
    "roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    concept_id=concept_id,\n",
    "    polygon=roi,\n",
    "    temporal=date_range,\n",
    "    count=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Filtering\n",
    "\n",
    "Now that we have some results, we will place them into a geodataframe that includes links to browse imagery and the files, so we can do some more advanced filtering of the data.\n",
    "\n",
    "List the metadata fields available in the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_metadata_fields(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some datasets have unique metadata that we can choose to include when we use our `results_to_geopandas` function to create a geodataframe. For example, `_cloud_cover` is not always available. We can add it to the default fields of this function by adding it to a `fields` argument in list form.\n",
    "\n",
    "default_fields = [  \n",
    "        \"size\",  \n",
    "        \"concept_id\",  \n",
    "        \"dataset-id\",  \n",
    "        \"native-id\",  \n",
    "        \"provider-id\",  \n",
    "        \"_related_urls\",  \n",
    "        \"_single_date_time\",  \n",
    "        \"_beginning_date_time\",  \n",
    "        \"_ending_date_time\",  \n",
    "        \"geometry\",  \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf = results_to_geopandas(results, fields=['_cloud_cover'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an index, so we can easily reference the data in the geodataframe and use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify index so we can reference it with gdf.explore()\n",
    "results_gdf['index']=results_gdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the results geodataframe by cloud cover. We'll use a cloud cover 10% as our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Results\n",
    "results_gdf = results_gdf[results_gdf['_cloud_cover'] < 10]\n",
    "results_gdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Figure and Basemap tiles\n",
    "fig = Figure(width=\"1080px\",height=\"540\")\n",
    "map1 = folium.Map(tiles=None)\n",
    "folium.TileLayer(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',name='Google Satellite', attr='Google', overlay=True).add_to(map1)\n",
    "folium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png',\n",
    "                name='ESRI World Imagery',\n",
    "                attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n",
    "                overlay='True').add_to(map1)\n",
    "fig.add_child(map1)\n",
    "# Add Search Results by Row'\n",
    "# Create a color map for the results\n",
    "cmap = cm.get_cmap('Set3')\n",
    "n = len(results_gdf['native-id'].unique())\n",
    "colors = [cmap(i) for i in range(n)]\n",
    "colors = [cm.colors.rgb2hex(color) for color in colors]\n",
    "\n",
    "for index, row in results_gdf.iterrows():\n",
    "    color = colors[index % len(colors)]\n",
    "    folium.GeoJson(row.geometry, name = row['native-id'],style_function=lambda feature, color=color: {'color': color, 'fillColor': color}).add_to(map1)\n",
    "\n",
    "# Add roi geodataframe\n",
    "roi_gdf.explore(\n",
    "    \"Name\",\n",
    "    popup=True,\n",
    "    categorical=True,\n",
    "    cmap='Set3',\n",
    "    style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n",
    "    name=\"Regions of Interest\",\n",
    "    m=map1\n",
    ")\n",
    "\n",
    "# Zoom to Data\n",
    "map1.fit_bounds(bounds=convert_bounds(results_gdf.unary_union.bounds))\n",
    "# Add Layer controls\n",
    "map1.add_child(folium.LayerControl(collapsed=False))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf._related_urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asset_url(row,asset, key='Type',value='GET DATA'):\n",
    "    \"\"\"\n",
    "    Retrieve a url from the list of dictionaries for a row in the _related_urls column.\n",
    "    Asset examples: CH4PLM, CH4PLMMETA, RFL, MASK, RFLUNCERT \n",
    "    \"\"\"\n",
    "    # Add _ to asset so string matching works\n",
    "    asset = f\"_{asset}_\"\n",
    "    # Retrieve URL matching parameters\n",
    "    for _dict in row['_related_urls']:\n",
    "        if _dict.get(key) == value and asset in _dict['URL'].split('/')[-1]:\n",
    "            return _dict['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows in the plm_gdf and get the mineral urls and store them in a list\n",
    "min_urls = results_gdf.apply(lambda row: get_asset_url(row, asset='L2B_MIN'), axis=1).tolist()\n",
    "min_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_png = results_gdf.apply(lambda row: get_asset_url(row, asset='L2B_MIN', value='GET RELATED VISUALIZATION'), axis=1).tolist()\n",
    "min_png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some knowledge of how the granules and assets are neamed, we can grab the rgb browse images to get an idea of what the location looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Collection ID\n",
    "rgb_urls = [s.replace('EMITL2BMIN', 'EMITL2ARFL') for s in min_png]\n",
    "# Update Product and Asset Names\n",
    "rgb_urls = [s.replace('EMIT_L2B_MIN', 'EMIT_L2A_RFL') for s in rgb_urls]\n",
    "# Change file extension\n",
    "#rgb_urls = [s.replace('.nc', '.png') for s in rgb_urls]\n",
    "rgb_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 3\n",
    "rows = math.ceil(len(results_gdf)/cols)\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(12,12))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for _n, index in enumerate(results_gdf.index.to_list()):\n",
    "    img = io.imread(rgb_urls[index])\n",
    "    ax[_n].imshow(img)\n",
    "    ax[_n].set_title(f\"Index: {index} - {results_gdf['native-id'][index]}\", fontsize=8)\n",
    "    ax[_n].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving Lists of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/rgb_browse_urls.txt', 'w') as f:\n",
    "    for line in rgb_urls:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/results_urls.txt', 'w') as f:\n",
    "    for line in min_urls:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming or Downloading Data\n",
    "For the workshop, we will stream the data, but either method can be used, and each has trade-offs based on the internet speed, storage space, or use case. The EMIT files are very large due to the number of bands, so operations can take some time if streaming with a slower internet connection. Since the workshop is hosted in a Cloud workspace, we can stream the data directly to the workspace.\n",
    "\n",
    "### 6.1 Streaming Data Workflow\n",
    "For an example of streaming both netCDF please see [Working with EMIT L2B Mineralogy.ipynb](Working_with_EMIT_L2B_Mineralogy.ipynb).\n",
    "\n",
    "If you plan to stream the data, you can stop here and move to the next notebook.\n",
    "\n",
    "### 6.2 Downloading Data Workflow\n",
    "To download the scenes, we can use the earthaccess library to authenticate then download the files.\n",
    "\n",
    "First, log into Earthdata using the login function from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. As mentioned in section 1.2, this step is not necessary to conduct searches, but is needed to download or stream data.\n",
    "\n",
    "The outputs saved in section 5 can be downloading by uncommenting and running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Authenticate using earthaccess\n",
    "# earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open Text File and Read Lines\n",
    "# file_list = ['../../data/rgb_browse_urls.txt','../../data/results_urls.txt']\n",
    "# urls = []\n",
    "# for file in file_list:\n",
    "#     with open(file) as f:\n",
    "#         urls.extend([line.rstrip('\\n') for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get requests https Session using Earthdata Login Info\n",
    "# fs = earthaccess.get_requests_https_session()\n",
    "# # Retrieve granule asset ID from URL (to maintain existing naming convention)\n",
    "# for url in urls:\n",
    "#     granule_asset_id = url.split('/')[-1]\n",
    "#     # Define Local Filepath\n",
    "#     fp = f'../../data/{granule_asset_id}'\n",
    "#     # Download the Granule Asset if it doesn't exist\n",
    "#     if not os.path.isfile(fp):\n",
    "#         with fs.get(url,stream=True) as src:\n",
    "#             with open(fp,'wb') as dst:\n",
    "#                 for chunk in src.iter_content(chunk_size=64*1024*1024):\n",
    "#                     dst.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Contact Info:  \n",
    "\n",
    "Email: LPDAAC@usgs.gov  \n",
    "Voice: +1-866-573-3222  \n",
    "Organization: Land Processes Distributed Active Archive Center (LP DAAC)¹  \n",
    "Website: <https://lpdaac.usgs.gov/>  \n",
    "Date last modified: 06-21-2024  \n",
    "\n",
    "¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpdaac_vitals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
